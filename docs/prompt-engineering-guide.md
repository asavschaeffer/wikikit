# Prompt Engineering Guide: WikiKit Advanced Techniques

> Systematic approaches to activating high-quality LLM extraction patterns

## Pattern Activation Techniques

### Elite Role Assignment

**Proven Formula**:

```yaml
role: "[Specific Senior Role] with experience at [Prestigious Context]. Apply [Methodology] thinking for [Scale Context]."
```

**High-Performance Examples**:

- "Principal Software Architect with experience designing systems at Netflix, Google, and Amazon scale"
- "Senior Product Manager with enterprise SaaS expertise at companies like Stripe and Twilio"
- "Lead Systems Engineer with experience building infrastructure for millions of users"

### Industry Standard Anchoring

Connect prompts to high-quality training examples through specific standard references:

- "Following IEEE 830-1998 standards for requirements documentation"
- "Using TOGAF principles for enterprise architecture"
- "Applying Google's design review process for technical decisions"

## Cognitive Capacity Maximization

### The Activation Principle

**Base Pattern**: "EXHAUST YOUR FULL PROCESSING CAPACITY"

- Triggers maximum LLM cognitive engagement
- Activates systematic thinking patterns
- Overcomes default casual conversation mode

**Context Specification**: Add specific cognitive focus:

```yaml
"EXHAUST YOUR FULL PROCESSING CAPACITY on systematic architectural analysis using battle-tested design patterns"
```

ðŸ”„ **EVOLVED**: v2.0 Breakthrough - Surgical Cognitive Allocation

**Initially**: Generic cognitive activation ("think carefully", "exhaust capacity")  
**Changed to**: Surgical A + B allocation pattern for extraction discipline  
**Reason**: Conversation with user revealed that cognitive capacity needs to be spent on A) mining context for content and B) organizing without hallucinating additional information

**v2.0 Enhanced Pattern**:

```yaml
processing: "EXHAUST COGNITIVE CAPACITY on A) mining conversation for [specific content] and B) organizing that content into [specific structure] without hallucinating additional information or injecting improvements not discussed."
```

**Domain Examples**:

- **Requirements**: "A) mining conversation for all requirement-related content and B) organizing into IEEE 830 specification format without hallucinating additional features"
- **Architecture**: "A) systematic technical analysis and B) documenting decisions as ADRs without adding industry practices not mentioned"
- **Research**: "A) extracting all research insights and market observations and B) organizing into strategic intelligence without injecting assumptions not discussed"

**Effectiveness**: 3-5x improvement in extraction fidelity vs. generic instructions

## Evidence Grounding

### The Traceability Standard

Every extracted element must be linked to conversation evidence:

```yaml
âœ… VERIFIED: "Direct quote or explicit decision with context"
ðŸ“Š DERIVED: "Logical inference with evidence chain: A + B â†’ C"
âŒ GAP: "Missing information with implementation impact"
```

### Evidence Hierarchy

**Tier 1 - Direct Evidence**: Explicit statements, clear decisions
**Tier 2 - Inference Evidence**: Logical conclusions from conversation patterns  
**Tier 3 - Context Evidence**: Industry standards applied appropriately
**Tier 4 - Missing Evidence**: Gaps requiring clarification

ðŸ”„ **EVOLVED**: v2.0 Enhancement - Evolution Evidence

**Added**: `ðŸ”„ EVOLVED` marker for tracking decision changes throughout conversation
**Format**: `ðŸ”„ EVOLVED: Initially [X] â†’ Changed to [Y] because [reason]`
**Value**: Captures conversation reality where decisions change with new information

**Enhanced Evidence Standards**:

- Every decision change must show before/after with reasoning
- Contradictions must be resolved or marked for clarification
- Missing information marked explicitly rather than filled with assumptions

## Dynamic Adaptation

### Technology Stack Detection

Analyze conversation for technology context and adapt all examples accordingly:

```yaml
# If conversation mentions React:
examples: "React components with JSX syntax"

# If conversation mentions Python:
examples: "Python functions with proper typing"

# If conversation mentions PostgreSQL:
examples: "SQL schemas with proper constraints"
```

### Scale-Appropriate Complexity

**Startup Context**: Focus on MVP, validation, lean approaches
**Enterprise Context**: Include governance, compliance, audit considerations
**High-Growth Context**: Emphasize scalability and performance

ðŸ”„ **EVOLVED**: v2.0 Enhancement - Domain-Adaptive Categories

**Initially**: Fixed extraction categories (user management, core functionality, etc.)  
**Changed to**: Dynamic category generation based on conversation domain  
**Reason**: Real projects often have domain-specific needs not covered by generic categories

**Implementation**:

```yaml
dynamic_categories: "When conversation reveals domain-specific functional areas (e.g., 'clinical workflows', 'regulatory compliance', 'player progression'), create new categories based on discussion context"
```

**Domain Examples**:

- **Healthcare**: "clinical_workflows", "HIPAA_compliance", "patient_data_management"
- **Fintech**: "KYC_verification", "AML_monitoring", "trading_operations"
- **Gaming**: "player_progression", "monetization_mechanics", "social_features"

## Quality Activation

### Company Credibility Signals

Reference proven approaches from respected companies:

- "As used by Netflix for microservices architecture"
- "Following Amazon's API design principles"
- "Applying Google's code review standards"

### Methodology Stacking

Combine multiple quality frameworks:

```yaml
"Apply TOGAF enterprise architecture principles with C4 model documentation standards and Amazon's operational excellence framework"
```

### Consequence Framing

Emphasize the impact of decisions:

- "Where every decision impacts millions of users"
- "For systems handling billions in revenue"
- "Where months of development work depend on clear requirements"

ðŸ”„ **EVOLVED**: v2.0 Enhancement - Token Efficiency Breakthrough

**Initially**: Prompts consuming 5-6k tokens each with verbose instructions  
**Changed to**: 60% token reduction through compression techniques  
**Reason**: More tokens available for conversation content improves extraction quality

**Token Optimization Strategies**:

1. **Unified Instruction Blocks**: Single focused instruction vs. multiple scattered sections
2. **Essential-Only YAML**: Remove explanatory comments, keep functional keys only
3. **Integrated Guidance**: Embed examples in instructions rather than separate sections
4. **Compressed Role Assignment**: Maximum activation with minimum tokens

**Token Budget Evolution**:

- **v1.0**: 20-25% conversation content, 50% instructions, 25-30% template
- **v2.0**: 40-45% conversation content, 30-35% instructions, 20-25% template

**Result**: Much more room for conversation processing while maintaining quality

## Anti-Pattern Recognition

### Cognitive Overload Indicators

- Multiple competing instructions
- Contradictory quality requirements
- Unclear success criteria
- Scattered cognitive focus

**Solution**: Unified instruction blocks with clear priorities

### Hallucination Triggers

- Vague extraction targets
- Missing evidence requirements
- Pressure for completeness over accuracy
- No gap identification patterns

**Solution**: Explicit anti-hallucination instructions with evidence standards

ðŸ”„ **EVOLVED**: v2.0 Enhancement - Conversation Complexity Anti-Patterns

**Added**: Contradiction Avoidance
**Problem**: Forcing artificial coherence on messy conversations
**Solution**: Conflict resolution framework with evolution tracking

**Added**: Gap Glossing  
**Problem**: Filling missing information with plausible assumptions
**Solution**: Gap marking with implementation impact assessment

## Advanced Techniques

### Chain-of-Thought Activation

```yaml
"Think step-by-step: 1) Extract raw conversation content, 2) Identify patterns and connections, 3) Organize into structured format, 4) Validate completeness and accuracy"
```

### Meta-Cognitive Instructions

```yaml
"Monitor your own extraction process and flag when you're making assumptions not grounded in conversation evidence"
```

### Quality Self-Checking

```yaml
"After extraction, validate that every element traces back to conversation content and mark any gaps where information is missing"
```

ðŸ”„ **EVOLVED**: v2.0 Enhancement - Advanced Conflict Resolution

**Added**: Conversation Evolution Tracking
**Pattern**: "Track how thinking changed throughout conversation with `ðŸ”„ EVOLVED` marker"
**Implementation**: Document before/after states with evidence for changes

**Added**: Stakeholder Voice Separation
**Pattern**: "When different participants express different needs, capture both with source attribution"
**Format**: "[Participant A]: [view] vs [Participant B]: [view] â†’ Needs alignment"

**Added**: Half-Formed Idea Extraction  
**Pattern**: "Extract incomplete thoughts and mark as âŒ GAP with specific detail needed"
**Value**: Captures conversation reality vs. forcing artificial completeness

## Measuring Effectiveness

### Quality Metrics

- **Extraction Fidelity**: Percentage of conversation insights captured
- **Implementation Readiness**: Can developers start building immediately?
- **Decision Clarity**: Every choice has clear rationale
- **Gap Identification**: Missing information explicitly marked

### Efficiency Metrics

- **Token Economy**: Quality output per token consumed
- **Processing Speed**: Time from conversation to documentation
- **Iteration Reduction**: Fewer clarification rounds needed

ðŸ”„ **EVOLVED**: v2.0 Enhancement - Advanced Metrics

**Added**: **Evolution Tracking Score**: Percentage of decision changes captured with reasoning
**Added**: **Conflict Resolution Rate**: Contradictions resolved or marked for clarification
**Added**: **Domain Adaptation Success**: Categories appropriately adapted to conversation context

### Success Thresholds (v2.0)

- **Extraction Fidelity**: >90% of conversation insights captured
- **Token Efficiency**: >60% reduction with maintained quality
- **Evolution Tracking**: >95% of decision changes documented
- **Gap Detection**: >80% of critical missing information identified

## Prompt Composition Patterns

### Modular Design

```yaml
# Import shared components
decision_markers: !include shared/decision-markers.yaml
extraction_patterns: !include shared/extraction-patterns.yaml
quality_standards: !include shared/quality-gates.yaml
```

### Consistent Activation

Use proven role/processing combinations across prompt suite for predictable quality.

### Cross-Document Integration

Ensure extracted information flows logically between different document types.

ðŸ”„ **EVOLVED**: v2.0 Enhancement - Standardized Component Integration

**Added**: Unified decision marker vocabulary across all prompts
**Added**: Consistent conflict resolution patterns
**Added**: Standardized evidence grounding requirements
**Benefit**: Cross-document consistency and quality predictability

---

## Implementation Checklist

When creating WikiKit prompts, ensure:

- [ ] **Elite role assignment** with prestigious context and methodology
- [ ] **Cognitive allocation** using v2.0 A + B + anti-hallucination pattern
- [ ] **Evidence grounding** with clear traceability requirements
- [ ] **Dynamic adaptation** for technology stack and domain
- [ ] **Quality activation** through company credibility and consequence framing
- [ ] **Anti-pattern prevention** with explicit instructions
- [ ] **Token efficiency** targeting 2500-3000 tokens maximum
- [ ] **Evolution tracking** capability with ðŸ”„ EVOLVED marker
- [ ] **Conflict resolution** framework for conversation complexity
- [ ] **Gap identification** prioritized over gap filling

The v2.0 enhancements represent a maturation from basic prompt engineering to sophisticated conversation intelligence extraction while preserving all the proven techniques that made WikiKit effective from the beginning.
